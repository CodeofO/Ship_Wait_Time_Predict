import pandas as pd
import numpy as np
import os
from sklearn.preprocessing import LabelEncoder, StandardScaler
import matplotlib.pyplot as plt
from sklearn.model_selection import StratifiedKFold, GridSearchCV, KFold
from sklearn.metrics import mean_absolute_error


'''
NOTE

í¬ê²Œ Preprocessing, Modeling ëª¨ë“ˆë¡œ ë‚˜ëˆ„ì–´ì§

1. Preprocessing(ì „ì²˜ë¦¬)
    1) feature engineering(1)
    2) bining(1)
    3) feature engineering(2)
    4) feature engineering(3)
    5) feature engineering(4)
    6) bining(2)
    7) drop columns(1)
    8) encoding
    9) drop columns(2)
    10) scaling

2. Modeling
    1) ë°ì´í„°ì…‹ ë¶„í• 
    2) K-Fold ë°ì´í„°ì…‹ ë¶„í•  ê¸°ì¤€ ìƒì„±(1)
    3) K-Fold ë°ì´í„°ì…‹ ë¶„í•  ê¸°ì¤€ ìƒì„±(2)
    4) ë³€ìˆ˜ ì¤‘ìš”ë„ ì¶”ì¶œ ë° ì‹œê°í™”, ì „ì²˜ë¦¬
    5) ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©
    6) Stratified K-Fold ê²€ì¦

ê° ëª¨ë“ˆë³„ ì„¤ëª…ì€ ì£¼ì„ìœ¼ë¡œ ì½”ë“œ ìœ„ì— ì‘ì„±
    
'''


# NOTE : Preprocessing 1 / feature engineering
# datetime > year, month, day, hour, minute, weekday
def pre_1_datetime(data):
    data['ATA'] = pd.to_datetime(data['ATA'])

    # datetimeì„ ì—¬ëŸ¬ íŒŒìƒ ë³€ìˆ˜ë¡œ ë³€í™˜
    data['year'] = data['ATA'].dt.year
    data['month'] = data['ATA'].dt.month
    data['day'] = data['ATA'].dt.day
    data['hour'] = data['ATA'].dt.hour
    data['minute'] = data['ATA'].dt.minute
    data['weekday'] = data['ATA'].dt.weekday
    return data





# NOTE : Preprocessing 2 / bining
# ìš”ì¼ -> ì£¼ì¤‘(0) / ì£¼ë§(1)ë¡œ ë³€ê²½
# weekdayëŠ” ì‚­ì œ
# DIST == 0 ì¼ ê²½ìš°ì—ë§Œ ì ìš©

def pre_2_weekday(data):

    weekday_binded = list()

    for day in data['weekday']:
        if day < 5:
            weekday_binded.append(0)
        else:
            weekday_binded.append(1)

    data['weekday_binded'] = weekday_binded

    return data





# NOTE : Preprocessing 3 / feature engineering
# CN(ì¤‘êµ­)ê³¼ TW(íƒ€ì´ì™„)ì— ë™ì¼í•œ ì´ë¦„ì„ ê°€ì§„ í•­êµ¬ëª… ì¡´ì¬ : EKP8
# EKP8_CN, EKP8_TWë¡œ ë¶„ë¦¬ì‹œí‚´
def pre_3_ARI_PORT(data):
    
    cond_EKP8 = data['ARI_PO'] == 'EKP8'
    cond_CN = data['ARI_CO'] == 'CN'
    cond_TW = data['ARI_CO'] == 'TW'

    data.loc[(cond_EKP8 & cond_CN), 'ARI_PO'] = 'EKP8_CN'
    data.loc[(cond_EKP8 & cond_TW), 'ARI_PO'] = 'EKP8_TW'

    return data





# NOTE : Preprocessing 4 / feature engineering
# ì„ ë°• ì „ì²´ ê¹Šì´ ì¤‘ ë¬¼ì— ì ê²¨ìˆëŠ” ê¹Šì´ì˜ ë¹„ìœ¨ : í˜ìˆ˜ ë†’ì´ / ì„ ë°•ì˜ ê¹Šì´
# DRAUGHT : í˜ìˆ˜ ë†’ì´(ì„ ì²´ê°€ ë¬¼ì— ì ê¸´ ê¹Šì´)
# DEPTH : ì„ ë°•ì˜ ê¹Šì´
def pre_4_ratio_under_water(data):

    data['ratio_under_water'] = 0.0

    cond = (data['DRAUGHT'] != 0) & (data['DEPTH'] != 0)
    data.loc[cond, 'ratio_under_water'] = np.round(data.loc[cond, 'DRAUGHT'] / data.loc[cond, 'DEPTH'], 2)
    data.loc[~cond, 'ratio_under_water'] = np.round((data.loc[~cond, 'DRAUGHT']+1) / (data.loc[~cond, 'DEPTH']+1), 2)
    
    return data





# NOTE : Preprocessing 5 / feature engineering
# ì„ ë°•ì˜ ì „ì²´ ì¬í™”ì¤‘ëŸ‰í†¤ìˆ˜ë¥¼ ë¬¼ì— ì ê²¨ìˆëŠ” ê¹Šì´ì˜ ë¹„ìœ¨ë§Œí¼ ê³±í•œ ìˆ˜ì¹˜ : (í˜ìˆ˜ ë†’ì´ / ì„ ë°•ì˜ ê¹Šì´) * ì¬í™”ì¤‘ëŸ‰í†¤ìˆ˜
# DRAUGHT : í˜ìˆ˜ ë†’ì´(ì„ ì²´ê°€ ë¬¼ì— ì ê¸´ ê¹Šì´)
# DEPTH : ì„ ë°•ì˜ ê¹Šì´
# DEADWEIGHT : ì„ ë°•ì˜ ì¬í™”ì¤‘ëŸ‰í†¤ìˆ˜(ì„ ë°•ì´ ì‹¤ì„ ìˆ˜ ìˆëŠ” í™”ë¬¼ì˜ ìµœëŒ€ ì¤‘ëŸ‰ì„ í†¤ìœ¼ë¡œ ë‚˜íƒ€ë‚¸ ìˆ˜ì¹˜)
def pre_5_ratio_under_water_DEADWEIGHT(data):

    data['ratio_under_water_DEAD'] = 0.0

    cond = (data['DRAUGHT'] != 0) & (data['DEPTH'] != 0)
    data.loc[cond, 'ratio_under_water_DEAD'] = np.round(data.loc[cond, 'DRAUGHT'] / data.loc[cond, 'DEPTH'], 2) * data.loc[cond, 'DEADWEIGHT']
    data.loc[~cond, 'ratio_under_water_DEAD'] = np.round((data.loc[~cond, 'DRAUGHT']+1) / (data.loc[~cond, 'DEPTH']+1), 2) * data.loc[~cond, 'DEADWEIGHT']
    
    return data





# NOTE : Preprocessing 6 / ë³€ìˆ˜ êµ¬ê°„í™”
# EDAê¸°ë°˜ ì–»ì€ ì¸ì‚¬ì´íŠ¸ë¥¼ í† ëŒ€ë¡œ êµ¬ê°„í™”ëœ DIST
#  0~75 / 75~150 / 150~175 / 175~ ë¡œ êµ¬ê°„í™” ëœë‹¤.
def pre_6_dist_binding(data):

    dist_l = list()

    for d in data['DIST']:

        if d < 75:
            dist_l.append(str('00'))
        elif d < 150:
            dist_l.append(str('01'))
        elif d < 175:
            dist_l.append(str('02'))
        else:
            dist_l.append(str('03'))
       
    data['DIST_binded'] = dist_l

    return data





# NOTE : Preprocessing 7 / ë³€ìˆ˜ ì œê±°
# drop : 'ID', 'SAMPLE_ID', 'SHIPMANAGER', 'ATA', 'FLAG'
    # feature importance(í”¼ì³ì¤‘ìš”ë„) í™•ì¸ í›„ ëª¨ë¸ ì„±ëŠ¥ì— ì˜í–¥ì„ ê±°ì˜ ë¯¸ì¹˜ì§€ ì•ŠëŠ” ë³€ìˆ˜ë“¤
    # drop í–ˆì„ ë•Œ ì„±ëŠ¥ì´ í–¥ìƒë˜ëŠ” ë³€ìˆ˜ë“¤
def pre_7_drop(data):
    
    cols_drop = ['ID', 'SAMPLE_ID', 'SHIPMANAGER', 'ATA', 'FLAG']
    data = data.drop(cols_drop, axis=1)
    
    return data




# NOTE : Preprocessing 8 / encoding
# ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë¥¼ ì œì™¸í•œ ë²”ì£¼í˜• ë³€ìˆ˜ ëŒ€ìƒìœ¼ë¡œ Label Encoding ìˆ˜í–‰\
# Label Encoding : ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ì—°ì†ë˜ëŠ” ìˆ«ìë¡œ ë³€í™˜í•¨(0ë¶€í„° ì‹œì‘)
def pre_8_encoding(train, test):

    # columns ë¦¬ìŠ¤íŠ¸ ìƒì„±
    cols_total = train.columns.tolist()

    # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ì´ë¦„ë§Œ ìˆëŠ” list
    cols_numeric = train.describe().columns.tolist()

    # ë²”ì£¼í˜• ë³€ìˆ˜ë§Œ ë¦¬ìŠ¤íŠ¸(cols_total)ì— ë‚¨ê¸°ê¸°
    for col in cols_numeric:
        cols_total.remove(col)

    cols_cate = cols_total

    # Labelencoding ìˆ˜í–‰
    encoder_le = LabelEncoder()

    for col in cols_cate:
        train[col] = encoder_le.fit_transform(train[col])
        test[col] = encoder_le.transform(test[col])

    return train, test





# NOTE : Preprocessing 9 / ë³€ìˆ˜ ì œê±°
# DISTê°€ 0ì¸ ë°ì´í„°ì…‹ì—ì„œë§Œ í•œì •í•´ì„œ ì œê±°
# í›„ì§„ ë³€ìˆ˜ì„ íƒë²•ì„ í†µí•œ ì‹¤í—˜ìœ¼ë¡œ ì•Œê²Œë¨
def pre_9_drop(data):
    
    cols_drop = ['weekday']
    data = data.drop(cols_drop, axis=1)
    
    return data




# NOTE : Preprocessing 10 / Scaling
# í•™ìŠµ ì „ ìµœì¢… ì „ì²˜ë¦¬
# X : Standard Scaling ìˆ˜í–‰
# Y : target(CI_HOUR)ë³€ìˆ˜ì— ln(1+x) ë³€í™˜ ìˆ˜í–‰
def pre_10_scaling(train, test):

    train_copy = train.copy()
    test_copy = test.copy()

    train_y = np.log1p(train_copy['CI_HOUR'])
    train_x = train_copy.drop(['CI_HOUR'], axis=1)

    scaler = StandardScaler()
    train_x[train_x.columns] = scaler.fit_transform(train_x)
    test_copy[test_copy.columns] = scaler.transform(test_copy)

    return train_x, train_y, test_copy





# NOTE : Modeling 1 / ë°ì´í„°ì…‹ ë¶„í• 
# DISTê°€ 0ì¸ samples, 0ì´ ì•„ë‹Œ samplesë¡œ ë¶„í• 
# ë¶„í• ì´ìœ  
    #: EDAê²°ê³¼ DISTê°€ 0ì¸ ê²½ìš° ëŒ€ë¶€ë¶„ target(CI_HOUR)ê°€ 0ì´ì˜€ìŒ.
    #  ë”°ë¼ì„œ DISTê°€ 0ì´ ì•„ë‹Œ ê²½ìš°ëŠ” DISTê°€ 0ì¸ ìƒí™©ì„ ì œì™¸í•˜ê³  í•™ìŠµí•˜ëŠ”ê²Œ ì¢‹ë‹¤ê³  íŒë‹¨.
def model_1_split_DIST(train, test):

    train_0 = train[train['DIST'] == 0]
    train_1 = train[train['DIST'] != 0]

    test_0 = test[test['DIST'] == 0]
    test_1 = test[test['DIST'] != 0]

    print(f'DIST = 0(train) : {train_0.shape}')
    print(f'DIST != 0(train) : {train_1.shape}')

    return train_0, train_1, test_0, test_1





# NOTE : Modeling 2 / K-Fold ë°ì´í„°ì…‹ ë¶„í•  ê¸°ì¤€ ìƒì„±(1)
# train_0, test_0 : target(CI_HOUR)ì´ 24ì‹œê°„ ë¯¸ë§Œì´ë©´ 0, ì´ìƒì´ë©´ 1
# train_1, test_1 : target(CI_HOUR)ì´ 100ì‹œê°„ ë¯¸ë§Œì´ë©´ 0, ì´ìƒì´ë©´ 1
# 24ì‹œê°„, 100ì‹œê°„ ì„ ì • ê·¼ê±°
    # 24ì‹œê°„(DIST=0) : EDA ê²°ê³¼ DISTê°€ 0ì¼ë•Œ 24ì‹œê°„ ë„˜ì–´ê°€ëŠ” sampleì€ 6ê°œê°€ ì¡´ì¬
    # 100ì‹œê°„(DIST!=0) : EDA ê²°ê³¼ CH_HOURê°€ 100ì´ìƒì´ ë  ë•Œ Bulk ì„ ë°•ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ì„ ë°•ì¢…ë¥˜ì˜ ë¶„í¬ê°€ 0ì— ìˆ˜ë ´í•¨
# ì¶”í›„ K-Fold êµì°¨ ê²€ì¦ì‹œ ì„ ë°• ì¢…ë¥˜, í•­êµ¬ëª…ê³¼ í•¨ê»˜ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©

def modeling_2_creterion_KF(train, creterion_num):

    # ë°ì´í„°í”„ë ˆì„ ë³µì‚¬
    train_copy = train.copy()

    # 'CI_HOUR_binded' ì—´ ì¶”ê°€
    train_copy['CI_HOUR_binded'] = train_copy['CI_HOUR'].apply(lambda x: '0' if x < creterion_num else '1')

    return train_copy





# NOTE : Modeling 3 / K-Fold ë°ì´í„°ì…‹ ë¶„í•  ê¸°ì¤€ ìƒì„±(2)
# í•­êµ¬ëª…, ì„ ë°• ì¢…ë¥˜, CI_HOUR_binded(êµ¬ê°„í™”ëœ target)ì„ ì‚¬ìš©í•´ K-Fold ê¸°ì¤€ ìƒì„±
# train_0(DIST=0)ì˜ ê¸°ì¤€ : í•­êµ¬ëª…, ì„ ë°• ì¢…ë¥˜, CI_HOUR_bindedì´ Feature Engineeringëœ ê²ƒ
# train_1(DIST!=0)ì˜ ê¸°ì¤€ : í•­êµ¬ëª…, ì„ ë°• ì¢…ë¥˜, CI_HOUR_binded, DIST_bindedì´ Feature Engineeringëœ ê²ƒ
def modeling_3_make_creterion(train_0, train_1):
    # make split cond
    train_0['SPLIT_CRETERION'] = train_0['ARI_PO'].astype(str) + '-' +train_0['SHIP_TYPE_CATEGORY'].astype(str)
    SPLIT_CRETERION_0 = train_0['SPLIT_CRETERION']
    train_0.drop(['SPLIT_CRETERION', 'CI_HOUR_binded'], axis=1, inplace=True)

    train_1['SPLIT_CRETERION'] = train_1['ARI_CO'].astype(str) + '-' +train_1['SHIP_TYPE_CATEGORY'].astype(str) + '-' +train_1['DIST_binded'].astype(str)#+ '-' +train_1['CI_HOUR_binded_1'].astype(str) 
    SPLIT_CRETERION_1 = train_1['SPLIT_CRETERION']
    train_1.drop(['SPLIT_CRETERION', 'CI_HOUR_binded'], axis=1, inplace=True)

    return SPLIT_CRETERION_0, SPLIT_CRETERION_1






# NOTE : Modeling 4 / ë³€ìˆ˜ ì¤‘ìš”ë„ ì¶”ì¶œ ë° ì‹œê°í™”, ì „ì²˜ë¦¬
# thresholdë³´ë‹¤ ì¤‘ìš”ë„ê°€ ë‚®ì€ columnì€ ì œê±°
# ì‹¤í—˜ì— ì˜í•´ threshold = 0.004ê°€ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œ
def modeling_4_feature_importance(train_x, train_y, test, model, model_name, threshold):

    print(f'Model Tune for {model_name}.')
    model.fit(train_x, train_y)

    feature_importances = model.feature_importances_
    sorted_idx = feature_importances.argsort()

    plt.figure(figsize=(10, len(train_x.columns) // 2.5))
    plt.title(f"Feature Importances ({model_name})")

    # Color distinction for features above vs below threshold
    for i, v in enumerate(feature_importances[sorted_idx]):
        plt.barh(i, v, align='center', color='blue' if v >= threshold else 'red')

    plt.yticks(range(train_x.shape[1]), train_x.columns[sorted_idx])
    plt.xlabel('Importance')
    plt.show()

    low_importance_features = train_x.columns[feature_importances < threshold]

    train_reduced = train_x.drop(columns=low_importance_features)
    test_reduced = test.drop(columns=low_importance_features)

    print('ğŸ’¡ ì œê±°ëœ columnë“¤ \n:', low_importance_features.tolist())

    return train_reduced, test_reduced, model, feature_importances





# NOTE : Modeling 5 / ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°
# Grid Searchë¥¼ ì‚¬ìš©í•´ ëª¨ë¸ì˜ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ë°œê²¬
def modeling_5_grid_search(train_x, train_y, model, param_grid):

    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    
    grid_search = GridSearchCV(model, param_grid, scoring='neg_mean_absolute_error', cv=kf, verbose=0, n_jobs=-1)
    grid_search.fit(train_x, train_y)

    best_model = grid_search.best_estimator_
    best_params = grid_search.best_params_
    
    return best_model, best_params




# NOTE : Modeling 6 / Stratified K-Fold
# best modelì— Stratified K-Foldì ìš© í›„ í•™ìŠµ, ê²€ì¦, ì¶”ë¡  ìˆ˜í–‰
def modeling_6_train_evaluation(train_x, train_y, test, best_model, SPLIT_CRETERION):

    model = best_model

    scores = []
    ensemble_predictions = []

    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    for fold, (train_index, test_index) in enumerate(skf.split(train_x, SPLIT_CRETERION)):
        train_set = train_x.iloc[train_index]
        valid_set = train_x.iloc[test_index]

        x_train, y_train = train_set, train_y.iloc[train_index]
        x_val, y_val = valid_set, train_y.iloc[test_index]

        model.fit(x_train, y_train, verbose=True)

        # ê° ëª¨ë¸ë¡œë¶€í„° Validation setì— ëŒ€í•œ ì˜ˆì¸¡ì„ í‰ê· ë‚´ì–´ ì•™ìƒë¸” ì˜ˆì¸¡ ìƒì„±
        val_pred = model.predict(x_val)


        # Validation setì— ëŒ€í•œ ëŒ€íšŒ í‰ê°€ ì‚°ì‹ ê³„ì‚° í›„ ì €ì¥
        # log scaling ë˜ëŒë¦¬ê¸°
        scores.append(mean_absolute_error(np.expm1(y_val), np.expm1(val_pred)))
        
        preds = model.predict(test)
        preds = np.where(preds < 0, 0, preds)

        ensemble_predictions.append(np.expm1(preds))

    # K-fold ëª¨ë“  ì˜ˆì¸¡ì˜ í‰ê· ì„ ê³„ì‚°í•˜ì—¬ foldë³„ ëª¨ë¸ë“¤ì˜ ì•™ìƒë¸” ì˜ˆì¸¡ ìƒì„±
    final_predictions = np.mean(ensemble_predictions, axis=0)

    # ê° foldì—ì„œì˜ Validation Metric Scoreì™€ ì „ì²´ í‰ê·  Validation Metric Scoreì¶œë ¥
    print('\n\nPrediction Score')
    print("Validation : MAE for each fold:", scores)
    print("Validation : MAE:", np.mean(scores))

    return final_predictions